{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf95b533",
   "metadata": {},
   "source": [
    "#### Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803bd0b",
   "metadata": {},
   "source": [
    "In order to differentiate between original and forged signatures a Siamese network was built which gives the similarity scores between pairs of signature images which are compared with each other to ensure authenticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a82f89",
   "metadata": {},
   "source": [
    "#### Import Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004f59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Input,GlobalAveragePooling2D,Dense,Lambda\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import split_folders\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f15c51",
   "metadata": {},
   "source": [
    "#### Building the model using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10a1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                32832     \n",
      "=================================================================\n",
      "Total params: 14,747,520\n",
      "Trainable params: 32,832\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to build the network architecture\n",
    "def build_siamese_model(input_shape,embedding_dim,fine_tune=False):\n",
    "    inputs=Input(input_shape)\n",
    "    preprocess_func=preprocess_input\n",
    "    base_model=VGG16(input_shape=input_shape,include_top=False)\n",
    "    \n",
    "    \n",
    "    # Training only the newly added Dense layers and freezing the rest of the layers\n",
    "    if fine_tune==False:\n",
    "        base_model.trainable=False\n",
    "        print(base_model.layers)\n",
    "    else:\n",
    "        base_model.trainable=True\n",
    "        \n",
    "        # Tune the model from this layer\n",
    "        fine_tune_from=len(base_model.layers)-1\n",
    "        \n",
    "        # Freezing all layers befor fine_tune\n",
    "        for layer in base_model.layers[:fine_tune_from]:\n",
    "            layer.trainable=False\n",
    "            \n",
    "    # Assigning new layers to the model to be trained\n",
    "    x=base_model(inputs)\n",
    "    x=GlobalAveragePooling2D()(x)\n",
    "    output=Dense(embedding_dim)(x)\n",
    "    model=Model(inputs,output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model=build_siamese_model((64,64,3),64,True)\n",
    "model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ce152",
   "metadata": {},
   "source": [
    "#### Creating Image pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b53f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(images,labels):\n",
    "    #Initialize 2 empty lists to hold pairs or images and lables to indiacte if the pair is positive(same) or negative (different)\n",
    "    pairImages=[]\n",
    "    pairLabels=[]\n",
    "    \n",
    "    #Total classes present in the dataset\n",
    "    num_class=len(np.unique(labels))\n",
    "    #Obtaining indexes of a given label\n",
    "    idx=[np.where(labels==i)[0] for i in range(0,num_class)]\n",
    "    \n",
    "    for index in range(len(images)):\n",
    "        #get image and label of the iteration\n",
    "        curr_image=images[index]\n",
    "        label=labels[index]\n",
    "        \n",
    "        #Randomly choose image belonging to same class\n",
    "        index_rand=np.random.choice(idx[label])\n",
    "        pos_image=images[index_rand]\n",
    "        \n",
    "        #Make positive pair and update the empty lists initialised\n",
    "        pairImages.append([curr_image,pos_image])\n",
    "        pairLabels.append([1])\n",
    "        \n",
    "        #Obtaining indexes of labels not equal to current label and its corresponding image\n",
    "        neg_idx=np.where(labels !=label)[0]\n",
    "        neg_image=images[np.random.choice(neg_idx)]\n",
    "        \n",
    "        #Make negative pair and update list\n",
    "        pairImages.append([curr_image,neg_image])\n",
    "        pairLabels.append([0])\n",
    "        \n",
    "    return (np.array(pairImages),np.array(pairLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a746fe5",
   "metadata": {},
   "source": [
    "#### Calculating Euclidean Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60147e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    # unpack the vectors into separate lists\n",
    "    (featsA, featsB) = vectors\n",
    "    # compute the sum of squared distances between the vectors\n",
    "    sumSquared = K.sum(K.square(featsA - featsB), axis=1,\n",
    "                       keepdims=True)\n",
    "    # return the euclidean distance between the vectors\n",
    "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ad358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d64ef5",
   "metadata": {},
   "source": [
    "#### Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb54405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 300 files [00:00, 799.70 files/s]\n"
     ]
    }
   ],
   "source": [
    "#Splitting the dataset into Train & Validation sets\n",
    "split_folders.ratio(\"D:/MTech-DSML/Main Projects/Signature Recognition_SiameseNW/sample_Signature/sample_Signature\",\"D:/MTech-DSML/Main Projects/Signature Recognition_SiameseNW/sample_Signature/output\",\n",
    "               seed=1337,\n",
    "    ratio=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa6e7af",
   "metadata": {},
   "source": [
    "#### Image Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92518da",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0744c577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_train=image_dataset_from_directory(\"D:/MTech-DSML/Main Projects/Signature Recognition_SiameseNW/sample_Signature/output/train/\", image_size=(64,64), batch_size=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e793ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 64, 64, 3)\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "for data_train, labels_train in data_train.take(1):\n",
    "    print(data_train.shape)\n",
    "    print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c514ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the train data\n",
    "data_train=data_train/255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d117e77",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db7301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data_val=image_dataset_from_directory(\"D:/MTech-DSML/Main Projects/Signature Recognition_SiameseNW/sample_Signature/output/val/\", image_size=(64,64), batch_size=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f23a3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 64, 64, 3)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "for data_val, labels_val in data_val.take(1):\n",
    "    print(data_val.shape)\n",
    "    print(labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fb12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the validation data\n",
    "data_val=data_val/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ceafcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data\n",
    "(pairTrain,labelTrain)=create_pairs(data_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44a72d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "(pairVal,labelVal)=create_pairs(data_val,labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ad3e8",
   "metadata": {},
   "source": [
    "#### Building a Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a5ac4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001F1562039B0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F14C378FD0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F15637B208>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F156575F98>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F15636D0F0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F156567390>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F1563D0278>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1563D0C18>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1565905C0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1565B8240>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F1565A6CF8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F15658ED30>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1565A48D0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1565C4A20>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F1565A42B0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F15658E358>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F1563C8320>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x000001F156567DA0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x000001F1565CA710>]\n"
     ]
    }
   ],
   "source": [
    "# Two inputs will pass through the network\n",
    "IMG_SHAPE=(64,64,3)\n",
    "img1 = Input(shape=IMG_SHAPE)\n",
    "img2 = Input( shape=IMG_SHAPE)\n",
    "featureExtractor = build_siamese_model(IMG_SHAPE,64)\n",
    "featsA = featureExtractor(img1)\n",
    "featsB = featureExtractor(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7f27709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The differencing layer to calculate the Euclidian distance between the two sister CNN networks encoding output.\n",
    "distance =Lambda(euclidean_distance)([featsA, featsB])\n",
    "outputs=Dense(1,activation='sigmoid')(distance)\n",
    "model = Model(inputs=[img1, img2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903e415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6199314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 1.4553 - accuracy: 0.4979 - val_loss: 1.1706 - val_accuracy: 0.4917\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 1.0280 - accuracy: 0.4979 - val_loss: 0.9912 - val_accuracy: 0.4917\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.8952 - accuracy: 0.4979 - val_loss: 0.9193 - val_accuracy: 0.4917\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.8287 - accuracy: 0.4979 - val_loss: 0.8757 - val_accuracy: 0.4917\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.7879 - accuracy: 0.4979 - val_loss: 0.8488 - val_accuracy: 0.4917\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 19s 1s/step - loss: 0.7607 - accuracy: 0.4979 - val_loss: 0.8332 - val_accuracy: 0.4917\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.7381 - accuracy: 0.4979 - val_loss: 0.8201 - val_accuracy: 0.4917\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.7234 - accuracy: 0.4979 - val_loss: 0.8117 - val_accuracy: 0.4917\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 19s 1s/step - loss: 0.7101 - accuracy: 0.4979 - val_loss: 0.8038 - val_accuracy: 0.4917\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6999 - accuracy: 0.4979 - val_loss: 0.7957 - val_accuracy: 0.4917\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6905 - accuracy: 0.4979 - val_loss: 0.7923 - val_accuracy: 0.4917\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6832 - accuracy: 0.4979 - val_loss: 0.7877 - val_accuracy: 0.4917\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.6771 - accuracy: 0.4979 - val_loss: 0.7842 - val_accuracy: 0.4917\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 22s 2s/step - loss: 0.6706 - accuracy: 0.4979 - val_loss: 0.7829 - val_accuracy: 0.4917\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6649 - accuracy: 0.4979 - val_loss: 0.7771 - val_accuracy: 0.4917\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6599 - accuracy: 0.4979 - val_loss: 0.7762 - val_accuracy: 0.4917\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6547 - accuracy: 0.4979 - val_loss: 0.7734 - val_accuracy: 0.4917\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.6507 - accuracy: 0.4979 - val_loss: 0.7718 - val_accuracy: 0.4917\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.6469 - accuracy: 0.4979 - val_loss: 0.7703 - val_accuracy: 0.4917\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6423 - accuracy: 0.4979 - val_loss: 0.7685 - val_accuracy: 0.4917\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6388 - accuracy: 0.5042 - val_loss: 0.7659 - val_accuracy: 0.4917\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6356 - accuracy: 0.5063 - val_loss: 0.7624 - val_accuracy: 0.4917\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 19s 1s/step - loss: 0.6317 - accuracy: 0.5063 - val_loss: 0.7668 - val_accuracy: 0.4917\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.6292 - accuracy: 0.5063 - val_loss: 0.7606 - val_accuracy: 0.4917\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.6263 - accuracy: 0.5063 - val_loss: 0.7646 - val_accuracy: 0.4917\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6220 - accuracy: 0.5063 - val_loss: 0.7604 - val_accuracy: 0.4917\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6193 - accuracy: 0.5063 - val_loss: 0.7601 - val_accuracy: 0.4917\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6153 - accuracy: 0.5104 - val_loss: 0.7599 - val_accuracy: 0.4917\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6139 - accuracy: 0.5146 - val_loss: 0.7593 - val_accuracy: 0.4917\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.6117 - accuracy: 0.5292 - val_loss: 0.7584 - val_accuracy: 0.4917\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 22s 1s/step - loss: 0.6083 - accuracy: 0.5229 - val_loss: 0.7584 - val_accuracy: 0.4917\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6045 - accuracy: 0.5354 - val_loss: 0.7575 - val_accuracy: 0.4917\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.6037 - accuracy: 0.5417 - val_loss: 0.7578 - val_accuracy: 0.4917\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.6017 - accuracy: 0.5437 - val_loss: 0.7566 - val_accuracy: 0.4917\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.5983 - accuracy: 0.5437 - val_loss: 0.7593 - val_accuracy: 0.4917\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.5974 - accuracy: 0.5521 - val_loss: 0.7563 - val_accuracy: 0.4917\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.5941 - accuracy: 0.5625 - val_loss: 0.7571 - val_accuracy: 0.4917\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5904 - accuracy: 0.5708 - val_loss: 0.7546 - val_accuracy: 0.4917\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5884 - accuracy: 0.5771 - val_loss: 0.7605 - val_accuracy: 0.4917\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.5871 - accuracy: 0.5813 - val_loss: 0.7571 - val_accuracy: 0.4917\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 23s 2s/step - loss: 0.5838 - accuracy: 0.5979 - val_loss: 0.7561 - val_accuracy: 0.4917\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 22s 2s/step - loss: 0.5827 - accuracy: 0.5979 - val_loss: 0.7563 - val_accuracy: 0.4917\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5811 - accuracy: 0.6042 - val_loss: 0.7586 - val_accuracy: 0.4917\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5788 - accuracy: 0.6187 - val_loss: 0.7566 - val_accuracy: 0.4917\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.5756 - accuracy: 0.6250 - val_loss: 0.7579 - val_accuracy: 0.4917\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 22s 2s/step - loss: 0.5714 - accuracy: 0.6542 - val_loss: 0.7583 - val_accuracy: 0.4917\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 24s 2s/step - loss: 0.5705 - accuracy: 0.6562 - val_loss: 0.7574 - val_accuracy: 0.4917\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 20s 1s/step - loss: 0.5680 - accuracy: 0.6542 - val_loss: 0.7605 - val_accuracy: 0.4917\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5667 - accuracy: 0.6687 - val_loss: 0.7575 - val_accuracy: 0.4917\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 21s 1s/step - loss: 0.5631 - accuracy: 0.6854 - val_loss: 0.7606 - val_accuracy: 0.4917\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "history = model.fit([pairTrain[:, 0], pairTrain[:, 1]], labelTrain[:], validation_data=([pairVal[:, 0], pairVal[:, 1]], labelVal[:]), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e144bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'],label='Train')\n",
    "plt.plot(history.history['val_loss'],label='Validation')\n",
    "plt.title(\"Train & Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdcb2e",
   "metadata": {},
   "source": [
    "#### Predict the Similarity score between 2 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "886ba946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7065986"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict([pairVal[1],pairVal[2]])\n",
    "preds.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a37d1a",
   "metadata": {},
   "source": [
    "#### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d87e9e",
   "metadata": {},
   "source": [
    "- Using binary cross entropy loss we get the similarity score between 2 images.\n",
    "- The best fit model gave a loss of 0.56 on Train data and 0.76 on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e66d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
